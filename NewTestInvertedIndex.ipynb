{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NewTestInvertedIndex.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **pip installs**"],"metadata":{"id":"PWqha9jPP8JY"}},{"cell_type":"code","source":["# These will already be installed in the testing environment so disregard the \n","# amount of time (~1 minute) it takes to install. \n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","\n","\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt-get update -qq\n","!apt install openjdk-8-jdk-headless -qq\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9-fM_yqZlVt","outputId":"ae2e2268-c561-4cdf-d84a-9cecd03d2ee6","executionInfo":{"status":"ok","timestamp":1641628827464,"user_tz":-120,"elapsed":108019,"user":{"displayName":"Shiri Itach","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04391784747690991477"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n","\u001b[K     |████████████████████████████████| 198 kB 43.9 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","The following additional packages will be installed:\n","  openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 36.5 MB of archives.\n","After this operation, 143 MB of additional disk space will be used.\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","(Reading database ... 155225 files and directories currently installed.)\n","Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","\u001b[K     |████████████████████████████████| 154 kB 5.2 MB/s \n","\u001b[?25hopenjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n","0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"]}]},{"cell_type":"markdown","source":["**code imports**"],"metadata":{"id":"j2x1qteWQCSV"}},{"cell_type":"code","source":["from collections import Counter, OrderedDict\n","from google.cloud import storage\n","from google.colab import auth\n","from graphframes import *\n","from inverted_index_colab import *\n","from itertools import islice, count, groupby\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import *\n","from operator import itemgetter\n","from pathlib import Path\n","from pyspark import SparkContext, SparkConf\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql import *\n","from pyspark.sql import SQLContext\n","from pyspark.sql.functions import *\n","from time import time\n","from timeit import timeit\n","import builtins\n","import hashlib\n","import itertools\n","import math\n","import nltk\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import pyspark\n","import re\n","import sys"],"metadata":{"id":"yK4jDbPsPwAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydKHb7SgSHog","outputId":"60397766-3624-4daa-cc8e-da6ccce9c8b6","executionInfo":{"status":"ok","timestamp":1641628829531,"user_tz":-120,"elapsed":671,"user":{"displayName":"Shiri Itach","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04391784747690991477"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-08 08:00:28--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n","Resolving repos.spark-packages.org (repos.spark-packages.org)... 13.226.124.26, 13.226.124.24, 13.226.124.67, ...\n","Connecting to repos.spark-packages.org (repos.spark-packages.org)|13.226.124.26|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 247880 (242K) [binary/octet-stream]\n","Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n","\n","graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.04s   \n","\n","2022-01-08 08:00:28 (5.98 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weiDv82CqoiU","outputId":"27e361ec-a0e7-45cc-a2c3-27ae1894e267","executionInfo":{"status":"ok","timestamp":1641628861613,"user_tz":-120,"elapsed":32098,"user":{"displayName":"Shiri Itach","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04391784747690991477"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Updated property [core/project].\n","\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [core-period-321814] or it does not exist.\n","\n","\n","To take a quick anonymous survey, run:\n","  $ gcloud survey\n","\n","Copying gs://wikidata_preprocessed/multistream1_preprocessed.parquet...\n","- [1 files][316.7 MiB/316.7 MiB]                                                \n","Operation completed over 1 objects/316.7 MiB.                                    \n"]}],"source":["def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","\n","# Initializing spark context\n","# create a spark context and session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","sc = pyspark.SparkContext(conf=conf)\n","sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Authenticate your user\n","# The authentication should be done with the email connected to your GCP account\n","auth.authenticate_user()\n","\n","# Copy one wikidumps files \n","\n","\n","project_id = 'core-period-321814'\n","!gcloud config set project {project_id}\n","\n","data_bucket_name = 'wikidata_preprocessed'\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","        pass  \n","except:\n","      !mkdir wikidumps\n","      !gsutil cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n"]},{"cell_type":"code","source":["from pathlib import Path \n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n","except:\n","      path = \"wikidumps/*\"\n","\n","parquetFile = spark.read.parquet(path)\n","\n","doc_title_pairs = parquetFile.limit(1000).select(\"title\", \"id\").rdd\n","doc_text_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n","doc_anchor_text_before = parquetFile.limit(1000).select(\"id\",\"anchor_text\").rdd\n","doc_anchor_text_pairs = (doc_anchor_text_before.map(lambda x: (map(lambda y: y[1],x[1]),x[0]))).map(lambda x: (\" \".join(x[0]),x[1]))\n"],"metadata":{"id":"3AZ8N3gWrEgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#parquetFile.show()"],"metadata":{"id":"PbuO7a4wmL2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(doc_title_pairs.take(5))"],"metadata":{"id":"rUb2VUioRTd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Code from assignment 3**\n","\n","---\n","\n"],"metadata":{"id":"nhcbR6pbserj"}},{"cell_type":"code","source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","\n","def doc_count(text, id):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  countTokens = OrderedDict(Counter(tokens))\n","  dict_keys = list(countTokens.keys())\n","  for token in dict_keys:\n","    if token in all_stopwords:\n","      countTokens.pop(token)\n","  countTokens_len = builtins.sum(countTokens.values())\n","  return (id,countTokens_len)\n","\n","def word_count(text, id):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  countTokens = OrderedDict(Counter(tokens))\n","  dict_keys = list(countTokens.keys())\n","  for token in dict_keys:\n","    if token in all_stopwords:\n","      countTokens.pop(token)\n","  return list(map(lambda x: (x , (id,countTokens[x])),countTokens.keys()))\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  return sorted(unsorted_pl, key=lambda x: x[1], reverse=True)\n","\n","def calculate_df(postings):\n","  return postings.map(lambda x: (x[0],len(x[1])))\n","\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings,basedir):\n","  newpostings = postings.map(lambda x: (token2bucket_id(x[0]),x))\n","  words2buckets = newpostings.groupByKey()\n","  x=words2buckets.map(lambda x: InvertedIndex.write_a_posting_list(x,basedir))\n","  return x\n","\n","def countForDict(posting_locs_list):\n","  super_posting_locs = defaultdict(list)\n","  for posting_loc in posting_locs_list:\n","    for k, v in posting_loc.items():\n","      super_posting_locs[k].extend(v)\n","  return super_posting_locs\n","\n","def createIndex(doc_rdd,directory):\n","  inverted = InvertedIndex()\n","  word_counts = doc_rdd.flatMap(lambda x: word_count(x[0], x[1]))\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","  #postings_filtered = postings.filter(lambda x: len(x[1])>10)\n","  df_ = calculate_df(postings)\n","  inverted.df = df_.collectAsMap()\n","  word_counter = postings.map(lambda x: (x[0], builtins.sum([y[1] for y in x[1]])))\n","  inverted.term_total = Counter(word_counter.collectAsMap())\n","  posting_locs_list = partition_postings_and_write(postings,directory).collect()\n","  inverted.posting_locs = countForDict(posting_locs_list)\n","  return inverted\n","\n","def writeIdx(inverted, directory, name):\n","  inverted.write_index(directory, name) \n","  inverted.write_dct(directory, name+'_dct') \n"],"metadata":{"id":"V13Ke81VsdiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from inverted_index_colab import *\n","!mkdir text title anchor"],"metadata":{"id":"OvneEK5mncEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #write to directories\n","title_index = createIndex(doc_title_pairs,'title')\n","title_index.dct=doc_title_pairs.map(lambda x: (x[1],x[0])).collectAsMap()\n","writeIdx(title_index, 'title','index_title')\n","\n","\n","text_index = createIndex(doc_text_pairs,'text')\n","word_counts_length = doc_text_pairs.map(lambda x: doc_count(x[0], x[1])) #only body\n","text_index.dct = word_counts_length.collectAsMap()  #only body\n","writeIdx(text_index, 'text','index_text')\n","\n","\n","anchor_index = createIndex(doc_anchor_text_pairs,'anchor')\n","writeIdx(anchor_index, 'anchor','index_anchor')"],"metadata":{"id":"gS5cCWF_sqkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader()) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list"],"metadata":{"id":"_AMKpsrUTdx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# t = InvertedIndex.read_posting_list(text_index,'movie')\n","# print(t)"],"metadata":{"id":"LWrIMU6G1VjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CosSim Code:**"],"metadata":{"id":"-Uk8d05TNvHc"}},{"cell_type":"code","source":["\n","def tokenize(text):\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens\n","\n","def get_posting_gen(index):\n","    words, pls = zip(*index.posting_lists_iter())\n","    return words, pls\n","\n","\n","def generate_query_tfidf_vector(query_to_search, index, DL):\n","    epsilon = .0000001\n","    total_vocab_size = len(index.term_total)\n","    Q = np.zeros(total_vocab_size)\n","    term_vector = list(index.term_total.keys())\n","    counter = Counter(query_to_search)\n","    for token in np.unique(query_to_search):\n","        if token in index.term_total.keys():  # avoid terms that do not appear in the index.\n","            tf = counter[token] / len(query_to_search)  # term frequency divded by the length of the query\n","            df = index.df.get(token, 0)\n","            idf = math.log((len(DL)) / (df + epsilon), 10)  # smoothing\n","\n","            try:\n","                ind = term_vector.index(token)\n","                Q[ind] = tf * idf\n","            except:\n","                pass\n","    return Q\n","\n","\n","def get_candidate_documents_and_scores(query_to_search, index, words, pls, DL):\n","    candidates = {}\n","    N = len(DL)\n","    for term in np.unique(query_to_search):\n","        if term in words:\n","            list_of_doc = pls[words.index(term)]\n","            normlized_tfidf = []\n","            for doc_id, freq in list_of_doc:\n","                normlized_tfidf.append((doc_id, (freq / DL[doc_id]) * math.log(N / index.df[term], 10)))\n","\n","            for doc_id, tfidf in normlized_tfidf:\n","                candidates[(doc_id, term)] = candidates.get((doc_id, term), 0) + tfidf\n","\n","    return candidates\n","\n","\n","def generate_document_tfidf_matrix(query_to_search, index, words, pls, DL):\n","    total_vocab_size = len(index.term_total)\n","    candidates_scores = get_candidate_documents_and_scores(query_to_search, index, words, pls, DL)\n","    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n","    D = np.zeros((len(unique_candidates), total_vocab_size))\n","    D = pd.DataFrame(D)\n","\n","    D.index = unique_candidates\n","    D.columns = index.term_total.keys()\n","\n","    for key in candidates_scores:\n","        tfidf = candidates_scores[key]\n","        doc_id, term = key\n","        D.loc[doc_id][term] = tfidf\n","    return D\n","\n","\n","def cosine_similarity(D, Q):\n","    cosSim_dict = {}\n","    docs = D.to_numpy()\n","    docid = 0\n","    for doc in docs:\n","        mone = np.dot(doc, Q)\n","        mechane = np.linalg.norm(doc) * np.linalg.norm(Q)\n","        cosSim = mone / mechane\n","        cosSim_dict[D.index[docid]] = cosSim\n","        docid += 1\n","    return cosSim_dict\n","\n","\n","def get_top_n(sim_dict):\n","    lst= sorted([(doc_id, builtins.round(score, 5)) for doc_id, score in sim_dict.items()], key=lambda x: x[1],\n","                  reverse=True)[:100]\n","    id_title_dct = InvertedIndex.read_index('title', 'index_title_dct')\n","    return [(int(x[0]), id_title_dct.get(x[0], 0)) for x in lst]\n","\n","\n","def get_topN_score_for_queries(clean_query, index):\n","    words, pls = get_posting_gen(index)\n","    DL = InvertedIndex.read_index('text', 'index_text_dct')\n","    Q = generate_query_tfidf_vector(clean_query, index, DL)\n","    D = generate_document_tfidf_matrix(clean_query, index, words, pls, DL)\n","    cosSim_dict = cosine_similarity(D, Q)\n","    topN = get_top_n(cosSim_dict)\n","    return topN"],"metadata":{"id":"bK9_2g4yNum5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean_queries=['similarity', 'laws', 'must', 'obeyed', 'constructing', 'aeroelastic', 'models', 'heated', 'high', 'speed', 'aircraft']\n","# idx_body =InvertedIndex.read_index('text','index_text')\n","# tfidf_queries_score = get_topN_score_for_queries(clean_queries,idx_body)\n","# #print(tfidf_queries_score)"],"metadata":{"id":"gG1zyzHHO-y3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Page Rank**"],"metadata":{"id":"vvGasv2_DSde"}},{"cell_type":"code","source":["def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  source = pages.map(lambda x: (x[0],))\n","  target = pages.flatMap(lambda x: map(lambda y: (y[0],) ,x[1]))\n","  vertices = (source.union(target)).distinct()\n","  edges = (pages.flatMap(lambda x: list(map(lambda y: (x[0],y[0]),x[1])))).distinct()\n","  return edges, vertices\n"],"metadata":{"id":"Of60usloOU0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# print(doc_anchor_text_before.take(5))\n","\n","# print(edges.take(5))\n","# print(vertices.take(5))"],"metadata":{"id":"_8zdGuWqLra5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**write dictionary to disk**"],"metadata":{"id":"MA5Q1Q3TMvpn"}},{"cell_type":"code","source":["# #doc_anchor_text_before = parquetFile.limit(1000).select(\"anchor_text\",\"id\").rdd\n","# edges, vertices = generate_graph(doc_anchor_text_before)\n","# edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n","# verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n","# g = GraphFrame(verticesDF, edgesDF)\n"],"metadata":{"id":"w2RTfgqlIC_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","# pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","# pr = pr.sort(col('pagerank').desc())\n","# wid2pr = {}\n","# for row in pr.rdd.toLocalIterator():\n","#     wid2pr[row[0]] = row[1]\n","  \n","\n","# with open('./pagerank', 'wb') as f:# write out the counter as binary file (pickle it)\n","#   pickle.dump(wid2pr, f)\n","# # a=pr.repartition(1).write.csv('test', compression=\"gzip\")\n","# # pr.show()"],"metadata":{"id":"a3M5jy6XNA3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# with open('./pagerank', 'rb') as f: # read in the dictionary from disk\n","#   wid2pr = pickle.loads(f.read())\n"],"metadata":{"id":"iJdDYLQ9O0bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(list(wid2pr.keys())[:5])\n","\n","# #part-00000-c7edd075-ec62-4d10-9694-d2a95242a2ea-c000.csv.gz\n","# #print(pr.repartition(1).read.csv('part-00000-c7edd075-ec62-4d10-9694-d2a95242a2ea-c000.csv.gz'))\n","# # df = spark.read.format(\"csv\").load(\"/content/test/part-00000-c7edd075-ec62-4d10-9694-d2a95242a2ea-c000.csv.gz\")\n","# # df.show()\n","# # df\n","# # print(df[2])"],"metadata":{"id":"oqCwDUVbA6yD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(pr.rdd.take(10)) # need to return some "],"metadata":{"id":"9qUGHSfoO5ke"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**search_title() and search_anchor()**\n","\n","---"],"metadata":{"id":"i1hRLF6GSRRs"}},{"cell_type":"code","source":["def search_by_binary(clean_query,index):\n","  words,pls = get_posting_gen(index)\n","  candidates = []\n","  for term in np.unique(clean_query):        \n","    if term in words:  \n","      candidates = candidates + list(map(lambda x: x[0], pls[words.index(term)]))\n","  in_order = list(map(lambda x: int(x[0]), (Counter(candidates)).most_common()))\n","  return in_order"],"metadata":{"id":"7vxhfZ_USWsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #test\n","# idx_anchor =InvertedIndex.read_index('anchor','index_anchor')\n","# rel_docs = search_by_binary([\"best\", \"marvel\", \"movie\", \"i\", \"ever\", \"love\", \"where\", \"does\", \"vanilla\", \"flavoring\" ,\"come\" ,\"from\", \"best\" ,\"ice\" ,\"cream\", \"flavour\"] , idx_anchor)\n","# print(rel_docs)"],"metadata":{"id":"ErwsKaj2SZIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hj5sgGY4eWLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Page views**"],"metadata":{"id":"GUUjyuCGpzXh"}},{"cell_type":"code","source":["# # Paths\n","# # Using user page views (as opposed to spiders and automated traffic) for the \n","# # month of August 2021\n","# pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","# p = Path(pv_path) \n","# pv_name = p.name\n","# pv_temp = f'{p.stem}-4dedup.txt'\n","# pv_clean = f'{p.stem}.pkl'\n","# # Download the file (2.3GB) \n","# !wget -N $pv_path\n","# # Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# # total number of page views (5). Then, remove lines with article id or page \n","# # view values that are not a sequence of digits.\n","# !bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp  #HORRIBLE DOWNLOAD\n","\n","# wid2pv = Counter() # Create a Counter (dictionary) that sums up the pages views for the same article, resulting in a mapping from article id to total page views.\n","# with open(pv_temp, 'rt') as f:\n","#   for line in f:\n","#     parts = line.split(' ')\n","#     wid2pv.update({int(parts[0]): int(parts[1])})\n","\n","# with open('./pageview', 'wb') as f:# write out the counter as binary file (pickle it)\n","#    pickle.dump(wid2pv, f)\n","\n","#FRONTEND\n","# with open('./pageview', 'rb') as f: # read in the dictionary from disk\n","#   wid22pv = pickle.loads(f.read())\n","\n","# keys = list(wid22pv.keys())\n","# for i in range(10):\n","#   key=keys[i]\n","#   print((key,wid22pv.get(key,0)))\n"],"metadata":{"id":"EeoTj_l3p43N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def most_viewed(wikiIDs):\n","#   lst=[]\n","#   for id in wikiIDs:\n","#     lst.append(wid22pv.get(id,0))\n","#   return (lst)"],"metadata":{"id":"ApjAzrJxqMbA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pages=[600744,59804426,4838455,7777] #list of doc id's need to write to disk\n","# print(pages)\n","# pages_ranked = most_viewed(pages)\n","# print(pages_ranked)"],"metadata":{"id":"pKRsIRUup9Kb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search Function\n","\n","---\n","\n","1. New indexes for title and body using stemming and remove stopwords, and?\n","2. bm25 data preprocess\n","3. bm25 calc and other calculations?\n","\n","4. נחשיב רק מסמכים שמכילים מושגים שה-idf שלהם גדול יותר מאיזשהו threashold שנגדיר\n","or - sort the posting list by tf-idf"],"metadata":{"id":"BE1zocZ5mDGe"}},{"cell_type":"code","source":["#    rdd:     doc_title_pairs   ,    doc_text_pairs\n","!mkdir search_text search_title"],"metadata":{"id":"a-O7bC1Wmo3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem.porter import *\n","stemmer = PorterStemmer()\n","\n","def improved_tokenize(text):\n","    list_of_tokens =  [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords] ##is it work?\n","    return list_of_tokens\n","\n","\n","def improved_word_count(text, id):\n","  tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower())]\n","  countTokens = OrderedDict(Counter(tokens))\n","  dict_keys = list(countTokens.keys())\n","  for token in dict_keys:\n","    if token in all_stopwords:\n","      countTokens.pop(token)\n","  return list(map(lambda x: (x , (id,countTokens[x])),countTokens.keys()))\n","\n","def improved_reduce_word_counts(unsorted_pl):\n","  return sorted(unsorted_pl, key=lambda x: x[1], reverse=True)[:100]\n","\n","\n","\n","def createAndWriteImprovedIndex(doc_rdd,directory,name):\n","  inverted = InvertedIndex()\n","  word_counts = doc_rdd.flatMap(lambda x: word_count(x[0], x[1]))\n","  postings = word_counts.groupByKey().mapValues(improved_reduce_word_counts)\n","  # #postings_filtered = postings.filter(lambda x: len(x[1])>10)\n","  df_ = calculate_df(postings)\n","  inverted.df = df_.collectAsMap()\n","  word_counter = postings.map(lambda x: (x[0], builtins.sum([y[1] for y in x[1]])))\n","  inverted.term_total = Counter(word_counter.collectAsMap())\n","  posting_locs_list = partition_postings_and_write(postings,directory).collect()  ##collect? \n","  inverted.posting_locs = countForDict(posting_locs_list)\n","  word_counts_length = doc_rdd.map(lambda x: doc_count(x[0], x[1]))\n","  inverted.dct = word_counts_length.collectAsMap()\n","  writeIdx(inverted, directory, name)\n","  return inverted\n"],"metadata":{"id":"0W3THIQpmCp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from BM25_from_index import *\n","\n","def search_back(clean_query):\n","  idx_title = InvertedIndex.read_index('search_title', 'index_title')\n","  idx_text = InvertedIndex.read_index('search_text', 'index_text')\n","  #topNtitle = get_topN_score_for_queries(clean_query,idx_title)\n","  #topNtext = get_topN_score_for_queries(clean_query,idx_text)\n","  dct_title = InvertedIndex.read_index('title', 'index_title_dct')\n","  DL_title =  InvertedIndex.read_index('search_title', 'index_title_dct')\n","  DL_text = InvertedIndex.read_index('search_text', 'index_text_dct')\n","  bm25title = BM25_from_index(idx_title,DL_title)\n","  bm25text = BM25_from_index(idx_text,DL_text)\n","  title_candidates = bm25title.search(clean_query)\n","  text_candidates = bm25text.search(clean_query)\n","  best_text_title = merge_results(title_candidates , text_candidates , N=100)\n","  return list(map(lambda tup: (int(tup[0]), dct_title[tup[0]]) , best_text_title))\n","  \n","\n","def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    \n","    # YOUR CODE HERE\n","    title_dict = dict(title_scores)\n","    body_dict = dict(body_scores)\n","    docs_id_for_query = set(list(title_dict.keys()) + list(body_dict.keys()))\n","    allscores = []\n","    for docid in docs_id_for_query:\n","      allscores.append((docid,title_dict.get(docid,0)*title_weight + body_dict.get(docid,0)*text_weight))\n","    score_sorted = sorted(allscores,key=lambda x:x[1],reverse = True)[:N]\n","    return score_sorted"],"metadata":{"id":"beOvELkoAs1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_index = createAndWriteImprovedIndex(doc_title_pairs,'search_title','index_title')\n","text_index = createAndWriteImprovedIndex(doc_text_pairs,'search_text','index_text')"],"metadata":{"id":"Mv3AoUc8Envn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = 'data science'\n","tokens = improved_tokenize(query)\n","print(search_back(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQOn4lgQEsHK","outputId":"9a0238dd-991c-4c49-e3f9-25f75c70a788","executionInfo":{"status":"ok","timestamp":1641629042039,"user_tz":-120,"elapsed":3066,"user":{"displayName":"Shiri Itach","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04391784747690991477"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(2349, 'Abstract data type'), (2052, 'Array data structure'), (2246, 'Analgesic'), (2380, 'Accelerated Graphics Port'), (1368, 'Assembly language'), (856, 'Apple Inc.'), (634, 'Analysis of variance'), (2316, 'Audio file format'), (1063, 'Algorithms for calculating variance'), (2078, 'Acorn Electron'), (775, 'Algorithm'), (586, 'ASCII'), (1451, 'APL (programming language)'), (1164, 'Artificial intelligence'), (612, 'Arithmetic mean'), (2230, 'Analysis of algorithms'), (1260, 'Advanced Encryption Standard'), (1094, 'Economy of Armenia'), (2141, 'Atari ST'), (1134, 'Analysis'), (2115, 'AppleTalk'), (1967, 'Apollo 12'), (1092, 'Demographics of Armenia'), (2039, 'Avionics'), (2116, 'Apple II series'), (308, 'Aristotle'), (1914, 'Antimicrobial resistance'), (1955, 'Adobe Inc.'), (1969, 'Apollo 15'), (1971, 'Apollo 17'), (569, 'Anthropology'), (2114, 'IBM AIX'), (2142, 'List of artificial intelligence projects'), (663, 'Apollo 8'), (1242, 'Ada (programming language)'), (738, 'Albania'), (1270, 'Extreme poverty'), (2308, 'Actinide'), (1902, 'American Airlines Flight 77'), (1397, 'AOL'), (2428, 'Analog computer'), (1456, 'AWK'), (1970, 'Apollo 16'), (1078, 'Antisemitism'), (2130, 'Aesthetics'), (698, 'Atlantic Ocean'), (748, 'Amateur astronomy'), (765, 'Abortion'), (2137, 'Aster CT-80'), (624, 'Alaska'), (662, 'Apollo 11'), (1210, 'Astronomical unit'), (1805, 'Antibiotic'), (303, 'Alabama'), (2362, 'Antibody'), (863, 'American Civil War'), (1461, 'Apollo program'), (1980, 'Amiga'), (25, 'Autism'), (580, 'Astronomer'), (2118, 'AVL tree'), (599, 'Afroasiatic languages'), (1140, 'Amplitude modulation'), (2170, 'ABCD'), (666, 'Alkali metal'), (1181, 'Astrometry'), (2208, 'Arctic fox'), (701, 'Angola'), (713, 'Android (robot)'), (2275, 'Apple II'), (1271, 'Analytical Engine'), (791, 'Asteroid'), (848, 'Audi'), (2388, 'Antidepressant'), (887, 'MessagePad'), (894, 'Agnosticism'), (897, 'Arsenic'), (1930, 'Arkansas'), (1968, 'Apollo 14'), (1525, 'Aspirin'), (1036, 'Aalborg Municipality'), (1069, 'Demographics of Antigua and Barbuda'), (1634, 'Aquaculture'), (1635, 'Kolmogorov complexity'), (1141, 'Augustin-Jean Fresnel'), (1653, 'Age of consent'), (655, 'Abacus'), (704, 'Demographics of Angola'), (1216, 'Athens'), (706, 'Economy of Angola'), (746, 'Azerbaijan'), (1274, 'Geography of Antarctica'), (786, 'Asparagales'), (1298, 'Ames, Iowa'), (1317, 'Antimatter'), (1349, 'Atanasoff–Berry computer'), (874, 'Ancient Egypt'), (901, 'Astatine'), (1418, 'Absolute zero'), (1494, 'Alfred Russel Wallace')]\n"]}]}]}