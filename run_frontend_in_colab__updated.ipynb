{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_frontend_in_colab__updated.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# download nltk stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "bCPDHP7zTQJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cee615-8234-488f-b055-4da1aab4f06f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough) \n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "lAt6KT8xOgHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fafff18-ce7b-4112-a3ee-b9f3bf0ea1e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 40 kB 35.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 51 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 61 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 71 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 81 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 92 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 102 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 106 kB 22.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 109 kB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.3.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "-oKFly5jFLFn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install ngrok to emulate public IP / address\n",
        "!wget -N https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip -u ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "FrBdFNYgiyab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea41cd3d-5f7f-4b15-97aa-54332ff60979"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2022-01-07 17:56:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 52.202.168.65, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  77.4MB/s    in 0.2s    \n",
            "\n",
            "2022-01-07 17:56:31 (77.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: sign up for an ngrok account then put your ngrok token below, uncomment, and execute\n",
        "!./ngrok authtoken 23JsrQWoovIqKcwTbcDwQMmpaX0_4fisNQVeiNo3QKpjqf4tX"
      ],
      "metadata": {
        "id": "xA-_dKEZbsXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0637683-9833-41ff-9288-237022b0b271"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nXn5PlyGORQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a421e379-19d9-404e-eb9c-f34df8e1496b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 30 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 40 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 51 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 61 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 71 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 81 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 83 kB 1.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install a ngrok python package and a version of flask that works with it in colab\n",
        "!pip -q install flask-ngrok\n",
        "!pip -q install flask==0.12.2\n",
        "# !pip -q install flask_restful"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUR INSTALLS**\n"
      ],
      "metadata": {
        "id": "fIXrn5ac7gfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "!apt-get update -qq\n",
        "!apt install openjdk-8-jdk-headless -qq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FygDq4ob7kQS",
        "outputId": "46cd9b05-3958-497c-aded-5993ad348ff7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 37 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 58.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hThe following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155225 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "\u001b[K     |████████████████████████████████| 154 kB 32.7 MB/s \n",
            "\u001b[?25hopenjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "from graphframes import *\n",
        "from inverted_index_colab import *\n",
        "from itertools import islice, count, groupby\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "from operator import itemgetter\n",
        "from pathlib import Path\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import *\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "import builtins\n",
        "import hashlib\n",
        "import itertools\n",
        "import math\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pyspark\n",
        "import re\n",
        "import sys"
      ],
      "metadata": {
        "id": "RZdlv-TbCb-k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**index creation**"
      ],
      "metadata": {
        "id": "Xq0lMeYIB8Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
        "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
        "!wget -N -P $spark_jars $graphframes_jar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MngwNWveC0Tj",
        "outputId": "9d16f4b7-dcd4-4e2e-c9ce-60f0b8dbc248"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-07 17:57:58--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 52.85.146.59, 52.85.146.12, 52.85.146.3, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|52.85.146.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247880 (242K) [binary/octet-stream]\n",
            "Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
            "\n",
            "\r          graphfram   0%[                    ]       0  --.-KB/s               \rgraphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-01-07 17:57:58 (36.8 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initializing spark context\n",
        "# create a spark context and session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "project_id = 'core-period-321814'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "data_bucket_name = 'wikidata_preprocessed'\n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "        pass  \n",
        "except:\n",
        "      !mkdir wikidumps\n",
        "      !gsutil cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYW8dDlHCrXM",
        "outputId": "158f5141-f753-439d-cbdb-167828d403d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Updated property [core/project].\n",
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [core-period-321814] or it does not exist.\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "Copying gs://wikidata_preprocessed/multistream1_preprocessed.parquet...\n",
            "/ [1 files][316.7 MiB/316.7 MiB]                                                \n",
            "Operation completed over 1 objects/316.7 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path \n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
        "except:\n",
        "      path = \"wikidumps/*\"\n",
        "\n",
        "parquetFile = spark.read.parquet(path)\n",
        "\n",
        "doc_title_pairs = parquetFile.limit(1000).select(\"title\", \"id\").rdd\n",
        "doc_text_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n",
        "doc_anchor_text_before = parquetFile.limit(1000).select(\"id\",\"anchor_text\").rdd\n",
        "doc_anchor_text_pairs = (doc_anchor_text_before.map(lambda x: (map(lambda y: y[1],x[1]),x[0]))).map(lambda x: (\" \".join(x[0]),x[1]))"
      ],
      "metadata": {
        "id": "4tJEXsMGCX3P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_colab import *\n",
        "!mkdir text title anchor\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "\n",
        "def doc_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  # YOUR CODE HERE\n",
        "  countTokens = OrderedDict(Counter(tokens))\n",
        "  dict_keys = list(countTokens.keys())\n",
        "  for token in dict_keys:\n",
        "    if token in all_stopwords:\n",
        "      countTokens.pop(token)\n",
        "  countTokens_len = builtins.sum(countTokens.values())\n",
        "  return (id,countTokens_len)\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  countTokens = OrderedDict(Counter(tokens))\n",
        "  dict_keys = list(countTokens.keys())\n",
        "  for token in dict_keys:\n",
        "    if token in all_stopwords:\n",
        "      countTokens.pop(token)\n",
        "  return list(map(lambda x: (x , (id,countTokens[x])),countTokens.keys()))\n",
        "\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl)\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.map(lambda x: (x[0],len(x[1])))\n",
        "\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def partition_postings_and_write(postings,basedir):\n",
        "  newpostings = postings.map(lambda x: (token2bucket_id(x[0]),x))\n",
        "  words2buckets = newpostings.groupByKey()\n",
        "  x=words2buckets.map(lambda x: InvertedIndex.write_a_posting_list(x,basedir))\n",
        "  return x\n",
        "def countForDict(posting_locs_list):\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for posting_loc in posting_locs_list:\n",
        "    for k, v in posting_loc.items():\n",
        "      super_posting_locs[k].extend(v)\n",
        "  return super_posting_locs\n",
        "\n",
        "\n",
        "def createIndex(doc_rdd,directory):\n",
        "  inverted = InvertedIndex()\n",
        "  word_counts = doc_rdd.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  postings_filtered = postings.filter(lambda x: len(x[1])>10)\n",
        "  df_ = calculate_df(postings_filtered)\n",
        "  inverted.df = df_.collectAsMap()\n",
        "  word_counter = postings.map(lambda x: (x[0], builtins.sum([y[1] for y in x[1]])))\n",
        "  inverted.term_total = Counter(word_counter.collectAsMap())\n",
        "  posting_locs_list = partition_postings_and_write(postings_filtered,directory).collect()\n",
        "  inverted.posting_locs = countForDict(posting_locs_list)\n",
        "  return inverted\n",
        "\n",
        "def writeIdx(inverted, directory, name):\n",
        "  inverted.write_index(directory, name) \n",
        "  inverted.write_dct(directory, name+'_dct') \n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "0hJAhHKBB_KQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "from BM25_from_index import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "!mkdir search_text search_title\n",
        "\n",
        "def improved_tokenize(text):\n",
        "    list_of_tokens =  [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords] ##is it work?\n",
        "    return list_of_tokens\n",
        "\n",
        "\n",
        "def improved_word_count(text, id):\n",
        "  tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower())]\n",
        "  countTokens = OrderedDict(Counter(tokens))\n",
        "  dict_keys = list(countTokens.keys())\n",
        "  for token in dict_keys:\n",
        "    if token in all_stopwords:\n",
        "      countTokens.pop(token)\n",
        "  return list(map(lambda x: (x , (id,countTokens[x])),countTokens.keys()))\n",
        "\n",
        "def improved_reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[1], reverse=True)[:100]\n",
        "\n",
        "\n",
        "def createImprovedIndex(doc_rdd,directory):\n",
        "  inverted = InvertedIndex()\n",
        "  word_counts = doc_rdd.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "  postings = word_counts.groupByKey().mapValues(improved_reduce_word_counts)\n",
        "  # #postings_filtered = postings.filter(lambda x: len(x[1])>10)\n",
        "  df_ = calculate_df(postings)\n",
        "  inverted.df = df_.collectAsMap()\n",
        "  word_counter = postings.map(lambda x: (x[0], builtins.sum([y[1] for y in x[1]])))\n",
        "  inverted.term_total = Counter(word_counter.collectAsMap())\n",
        "  posting_locs_list = partition_postings_and_write(postings,directory).collect()  ##collect? \n",
        "  inverted.posting_locs = countForDict(posting_locs_list)\n",
        "  return inverted\n",
        "\n",
        "\n",
        "\n",
        "def search_back(clean_query):\n",
        "  idx_title = InvertedIndex.read_index('search_title', 'index_title')\n",
        "  idx_text = InvertedIndex.read_index('search_text', 'index_text')\n",
        "  #topNtitle = get_topN_score_for_queries(clean_query,idx_title)\n",
        "  #topNtext = get_topN_score_for_queries(clean_query,idx_text)\n",
        "  dct_title = InvertedIndex.read_index('search_title', 'index_title_dct')\n",
        "  DL_title = dict(map(lambda tup: (tup[0],len(tup[1].split(' '))),dct_title.items()))\n",
        "  DL_text = InvertedIndex.read_index('search_text', 'index_text_dct')\n",
        "  bm25title = BM25_from_index(idx_title,DL_title)\n",
        "  bm25text = BM25_from_index(idx_text,DL_text)\n",
        "  title_candidates = bm25title.search(clean_query)\n",
        "  text_candidates = bm25text.search(clean_query)\n",
        "  best_text_title = merge_results(title_candidates , text_candidates , N=100)\n",
        "  return list(map(lambda tup: (tup[0], dct_title[tup[0]]) , best_text_title))\n",
        "  \n",
        "\n",
        "def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    \n",
        "    # YOUR CODE HERE\n",
        "    title_dict = dict(title_scores)\n",
        "    body_dict = dict(body_scores)\n",
        "    docs_id_for_query = set(list(title_dict.keys()) + list(body_dict.keys()))\n",
        "    allscores = []\n",
        "    for docid in docs_id_for_query:\n",
        "      allscores.append((docid,title_dict.get(docid,0)*title_weight + body_dict.get(docid,0)*text_weight))\n",
        "    score_sorted = sorted(allscores,key=lambda x:x[1],reverse = True)[:N]\n",
        "    return score_sorted"
      ],
      "metadata": {
        "id": "Vtckm5XHp1Ng"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_index = createIndex(doc_title_pairs,'title')\n",
        "title_index.dct=doc_title_pairs.map(lambda x: (x[1],x[0])).collectAsMap()\n",
        "writeIdx(title_index, 'title','index_title')\n",
        "\n",
        "\n",
        "text_index = createIndex(doc_text_pairs,'text')\n",
        "word_counts_length = doc_text_pairs.map(lambda x: doc_count(x[0], x[1])) #only body\n",
        "text_index.dct = word_counts_length.collectAsMap()  #only body\n",
        "writeIdx(text_index, 'text','index_text')\n",
        "\n",
        "\n",
        "anchor_index = createIndex(doc_anchor_text_pairs,'anchor')\n",
        "writeIdx(anchor_index, 'anchor','index_anchor')\n",
        "\n",
        "# title_index = createImprovedIndex(doc_title_pairs,'search_title')\n",
        "# title_index.dct=doc_title_pairs.map(lambda x: (x[1],x[0])).collectAsMap()\n",
        "# writeIdx(title_index, 'search_title','index_title')\n",
        "\n",
        "# text_index = createImprovedIndex(doc_text_pairs,'search_text')\n",
        "# word_counts_length = doc_text_pairs.map(lambda x: doc_count(x[0], x[1])) #only body\n",
        "# text_index.dct = word_counts_length.collectAsMap()  #only body\n",
        "# writeIdx(text_index, 'search_text','index_text')"
      ],
      "metadata": {
        "id": "3GytE-iSCJke"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the app"
      ],
      "metadata": {
        "id": "6dW0y91OVu5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to upload your implementation of search_app.py\n",
        "import search_frontend as se\n",
        "import search_backend as sb"
      ],
      "metadata": {
        "id": "7opNkV6uRHIv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment the code below and execute to reload the module when you make \n",
        "# changes to search_frontend.py (after you upload again).\n",
        "# import importlib\n",
        "# importlib.reload(sb)"
      ],
      "metadata": {
        "id": "oTGXXYEXV5l8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "run_with_ngrok(se.app)\n",
        "se.app.run()"
      ],
      "metadata": {
        "id": "J5n9u9rFP_wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03a8b66-4510-44d7-c77b-0831b742e768"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://476e-35-245-125-86.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            " * Running on http://476e-35-245-125-86.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [07/Jan/2022 18:01:02] \"\u001b[37mGET /search_body?query=dog HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:02:02] \"\u001b[37mGET /search_body?query=data+science HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:02:09] \"\u001b[37mGET /search_body?query=data+science HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:02:37] \"\u001b[37mGET /search_body?query=data+science HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:02:40] \"\u001b[37mGET /search?query=data+science HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:03:32] \"\u001b[37mGET /search_body?query=best+marvel+movie HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Jan/2022 18:03:36] \"\u001b[37mGET /search?query=best+marvel+movie HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing your app\n",
        "\n",
        "Once your app is running you can query it. You can simply do that by navigating to the URL that ngrok gave you above or through code in a different python session. For example, once the frontend app is running, you can navigate to:\n",
        "http://YOUR_SERVER_DOMAIN/search?query=hello+world where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io, which is printed above in Colab or that is your external IP on GCP.\n",
        "\n",
        "The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before."
      ],
      "metadata": {
        "id": "Na0MC_1nzDbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('queries_train.json', 'rt') as f:\n",
        "  queries = json.load(f)"
      ],
      "metadata": {
        "id": "EM5ePrRHojbG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    for i,doc_id in enumerate(predicted_list):        \n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)            \n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return round(sum(precisions)/len(precisions),3)"
      ],
      "metadata": {
        "id": "gWimZWCOy3Ei"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from time import time\n",
        "# url = 'http://35.232.59.3:8080'\n",
        "# place the domain you got from ngrok or GCP IP below. \n",
        "url = 'http://b4e3-34-86-221-19.ngrok.io'\n",
        "\n",
        "qs_res = []\n",
        "for q, true_wids in queries.items():\n",
        "  duration, ap = None, None\n",
        "  t_start = time()\n",
        "  try:\n",
        "    res = requests.get(url + '/search_body', {'query': q}, timeout=35)\n",
        "    duration = time() - t_start\n",
        "    if res.status_code == 200:\n",
        "      pred_wids, _ = zip(*res.json())\n",
        "      ap = average_precision(true_wids, pred_wids)\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  qs_res.append((q, duration, ap))"
      ],
      "metadata": {
        "id": "dYmNTq9u0ChK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(res.text)\n",
        "print(qs_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYeMEZNSBK36",
        "outputId": "62cedf90-63b5-4924-eb40-72d8b3025307"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('python', 0.09722089767456055, None), ('data science', 0.08135032653808594, None), ('migraine', 0.08765363693237305, None), ('chocolate', 0.07992172241210938, None), ('how to make pasta', 0.0807340145111084, None), ('Does pasta have preservatives?', 0.07966375350952148, None), ('how google works', 0.07768821716308594, None), ('what is information retrieval', 0.08137679100036621, None), ('NBA', 0.07975387573242188, None), ('yoga', 0.07917547225952148, None), ('how to not kill plants', 0.08975553512573242, None), ('masks', 0.08062624931335449, None), ('black friday', 0.0788264274597168, None), ('why do men have nipples', 0.0788261890411377, None), ('rubber duck', 0.07910823822021484, None), ('michelin', 0.07982730865478516, None), ('what to watch', 0.0831899642944336, None), ('best marvel movie', 0.0806124210357666, None), ('how tall is the eiffel tower', 0.07882547378540039, None), ('where does vanilla flavoring come from', 0.08446311950683594, None), ('best ice cream flavour', 0.08229947090148926, None), ('how to tie a tie', 0.08015727996826172, None), ('how to earn money online', 0.08065629005432129, None), ('what is critical race theory', 0.08282732963562012, None), ('what space movie was made in 1992', 0.07812952995300293, None), ('how to vote', 0.07825016975402832, None), ('google trends', 0.07729172706604004, None), ('dim sum', 0.0846092700958252, None), ('ted', 0.08051776885986328, None), ('fairy tale', 0.08007025718688965, None)]\n"
          ]
        }
      ]
    }
  ]
}